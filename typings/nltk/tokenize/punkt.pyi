"""
This type stub file was generated by pyright.
"""

from typing import Any, Dict, Iterator, List, Tuple
from nltk.tokenize.api import TokenizerI

r"""
Punkt Sentence Tokenizer

This tokenizer divides a text into a list of sentences
by using an unsupervised algorithm to build a model for abbreviation
words, collocations, and words that start sentences.  It must be
trained on a large collection of plaintext in the target language
before it can be used.

The NLTK data package includes a pre-trained Punkt tokenizer for
English.

    >>> from nltk.tokenize import PunktTokenizer
    >>> text = '''
    ... Punkt knows that the periods in Mr. Smith and Johann S. Bach
    ... do not mark sentence boundaries.  And sometimes sentences
    ... can start with non-capitalized words.  i is a good variable
    ... name.
    ... '''
    >>> sent_detector = PunktTokenizer()
    >>> print('\n-----\n'.join(sent_detector.tokenize(text.strip())))
    Punkt knows that the periods in Mr. Smith and Johann S. Bach
    do not mark sentence boundaries.
    -----
    And sometimes sentences
    can start with non-capitalized words.
    -----
    i is a good variable
    name.

(Note that whitespace from the original text, including newlines, is
retained in the output.)

Punctuation following sentences is also included by default
(from NLTK 3.0 onwards). It can be excluded with the realign_boundaries
flag.

    >>> text = '''
    ... (How does it deal with this parenthesis?)  "It should be part of the
    ... previous sentence." "(And the same with this one.)" ('And this one!')
    ... "('(And (this)) '?)" [(and this. )]
    ... '''
    >>> print('\n-----\n'.join(
    ...     sent_detector.tokenize(text.strip())))
    (How does it deal with this parenthesis?)
    -----
    "It should be part of the
    previous sentence."
    -----
    "(And the same with this one.)"
    -----
    ('And this one!')
    -----
    "('(And (this)) '?)"
    -----
    [(and this. )]
    >>> print('\n-----\n'.join(
    ...     sent_detector.tokenize(text.strip(), realign_boundaries=False)))
    (How does it deal with this parenthesis?
    -----
    )  "It should be part of the
    previous sentence.
    -----
    " "(And the same with this one.
    -----
    )" ('And this one!
    -----
    ')
    "('(And (this)) '?
    -----
    )" [(and this.
    -----
    )]

However, Punkt is designed to learn parameters (a list of abbreviations, etc.)
unsupervised from a corpus similar to the target domain. The pre-packaged models
may therefore be unsuitable: use ``PunktSentenceTokenizer(text)`` to learn
parameters from the given text.

:class:`.PunktTrainer` learns parameters such as a list of abbreviations
(without supervision) from portions of text. Using a ``PunktTrainer`` directly
allows for incremental training and modification of the hyper-parameters used
to decide what is considered an abbreviation, etc.

The algorithm for this tokenizer is described in::

  Kiss, Tibor and Strunk, Jan (2006): Unsupervised Multilingual Sentence
    Boundary Detection.  Computational Linguistics 32: 485-525.
"""
_ORTHO_BEG_UC = ...
_ORTHO_MID_UC = ...
_ORTHO_UNK_UC = ...
_ORTHO_BEG_LC = ...
_ORTHO_MID_LC = ...
_ORTHO_UNK_LC = ...
_ORTHO_UC = ...
_ORTHO_LC = ...
_ORTHO_MAP = ...
REASON_DEFAULT_DECISION = ...
REASON_KNOWN_COLLOCATION = ...
REASON_ABBR_WITH_ORTHOGRAPHIC_HEURISTIC = ...
REASON_ABBR_WITH_SENTENCE_STARTER = ...
REASON_INITIAL_WITH_ORTHOGRAPHIC_HEURISTIC = ...
REASON_NUMBER_WITH_ORTHOGRAPHIC_HEURISTIC = ...
REASON_INITIAL_WITH_SPECIAL_ORTHOGRAPHIC_HEURISTIC = ...
class PunktLanguageVars:
    """
    Stores variables, mostly regular expressions, which may be
    language-dependent for correct application of the algorithm.
    An extension of this class may modify its properties to suit
    a language other than English; an instance can then be passed
    as an argument to PunktSentenceTokenizer and PunktTrainer
    constructors.
    """
    __slots__ = ...
    def __getstate__(self): # -> Literal[1]:
        ...

    def __setstate__(self, state): # -> Literal[1]:
        ...

    sent_end_chars = ...
    internal_punctuation = ...
    re_boundary_realignment = ...
    _re_word_start = ...
    _re_multi_char_punct = ...
    _word_tokenize_fmt = ...
    def word_tokenize(self, s): # -> list[Any]:
        """Tokenize a string to split off punctuation other than periods"""
        ...

    _period_context_fmt = ...
    def period_context_re(self): # -> Pattern[str]:
        """Compiles and returns a regular expression to find contexts
        including possible sentence boundaries."""
        ...



_re_non_punct = ...
class PunktParameters:
    """Stores data used to perform sentence boundary detection with Punkt."""
    def __init__(self) -> None:
        ...

    def clear_abbrevs(self): # -> None:
        ...

    def clear_collocations(self): # -> None:
        ...

    def clear_sent_starters(self): # -> None:
        ...

    def clear_ortho_context(self): # -> None:
        ...

    def add_ortho_context(self, typ, flag): # -> None:
        ...



class PunktToken:
    """Stores a token of text with annotations produced during
    sentence boundary detection."""
    _properties = ...
    __slots__ = ...
    def __init__(self, tok, **params) -> None:
        ...

    _RE_ELLIPSIS = ...
    _RE_NUMERIC = ...
    _RE_INITIAL = ...
    _RE_ALPHA = ...
    @property
    def type_no_period(self): # -> str:
        """
        The type with its final period removed if it has one.
        """
        ...

    @property
    def type_no_sentperiod(self): # -> str:
        """
        The type with its final period removed if it is marked as a
        sentence break.
        """
        ...

    @property
    def first_upper(self):
        """True if the token's first character is uppercase."""
        ...

    @property
    def first_lower(self):
        """True if the token's first character is lowercase."""
        ...

    @property
    def first_case(self): # -> Literal['lower', 'upper', 'none']:
        ...

    @property
    def is_ellipsis(self): # -> Match[str] | None:
        """True if the token text is that of an ellipsis."""
        ...

    @property
    def is_number(self): # -> bool:
        """True if the token text is that of a number."""
        ...

    @property
    def is_initial(self): # -> Match[str] | None:
        """True if the token text is that of an initial."""
        ...

    @property
    def is_alpha(self): # -> Match[str] | None:
        """True if the token text is all alphabetic."""
        ...

    @property
    def is_non_punct(self): # -> Match[str] | None:
        """True if the token is either a number or is alphabetic."""
        ...

    def __repr__(self): # -> str:
        """
        A string representation of the token that can reproduce it
        with eval(), which lists all the token's non-default
        annotations.
        """
        ...

    def __str__(self) -> str:
        """
        A string representation akin to that used by Kiss and Strunk.
        """
        ...



class PunktBaseClass:
    """
    Includes common components of PunktTrainer and PunktSentenceTokenizer.
    """
    def __init__(self, lang_vars=..., token_cls=..., params=...) -> None:
        ...



class PunktTrainer(PunktBaseClass):
    """Learns parameters used in Punkt sentence boundary detection."""
    def __init__(self, train_text=..., verbose=..., lang_vars=..., token_cls=...) -> None:
        ...

    def get_params(self): # -> PunktParameters:
        """
        Calculates and returns parameters for sentence boundary detection as
        derived from training."""
        ...

    ABBREV = ...
    IGNORE_ABBREV_PENALTY = ...
    ABBREV_BACKOFF = ...
    COLLOCATION = ...
    SENT_STARTER = ...
    INCLUDE_ALL_COLLOCS = ...
    INCLUDE_ABBREV_COLLOCS = ...
    MIN_COLLOC_FREQ = ...
    def train(self, text, verbose=..., finalize=...): # -> None:
        """
        Collects training data from a given text. If finalize is True, it
        will determine all the parameters for sentence boundary detection. If
        not, this will be delayed until get_params() or finalize_training() is
        called. If verbose is True, abbreviations found will be listed.
        """
        ...

    def train_tokens(self, tokens, verbose=..., finalize=...): # -> None:
        """
        Collects training data from a given list of tokens.
        """
        ...

    def finalize_training(self, verbose=...): # -> None:
        """
        Uses data that has been gathered in training to determine likely
        collocations and sentence starters.
        """
        ...

    def freq_threshold(self, ortho_thresh=..., type_thresh=..., colloc_thres=..., sentstart_thresh=...): # -> None:
        """
        Allows memory use to be reduced after much training by removing data
        about rare tokens that are unlikely to have a statistical effect with
        further training. Entries occurring above the given thresholds will be
        retained.
        """
        ...

    def find_abbrev_types(self): # -> None:
        """
        Recalculates abbreviations given type frequencies, despite no prior
        determination of abbreviations.
        This fails to include abbreviations otherwise found as "rare".
        """
        ...



class PunktSentenceTokenizer(PunktBaseClass, TokenizerI):
    """
    A sentence tokenizer which uses an unsupervised algorithm to build
    a model for abbreviation words, collocations, and words that start
    sentences; and then uses that model to find sentence boundaries.
    This approach has been shown to work well for many European
    languages.
    """
    def __init__(self, train_text=..., verbose=..., lang_vars=..., token_cls=...) -> None:
        """
        train_text can either be the sole training text for this sentence
        boundary detector, or can be a PunktParameters object.
        """
        ...

    def train(self, train_text, verbose=...): # -> PunktParameters:
        """
        Derives parameters from a given training text, or uses the parameters
        given. Repeated calls to this method destroy previous parameters. For
        incremental training, instantiate a separate PunktTrainer instance.
        """
        ...

    def tokenize(self, text: str, realign_boundaries: bool = ...) -> List[str]:
        """
        Given a text, returns a list of the sentences in that text.
        """
        ...

    def debug_decisions(self, text: str) -> Iterator[Dict[str, Any]]:
        """
        Classifies candidate periods as sentence breaks, yielding a dict for
        each that may be used to understand why the decision was made.

        See format_debug_decision() to help make this output readable.
        """
        ...

    def span_tokenize(self, text: str, realign_boundaries: bool = ...) -> Iterator[Tuple[int, int]]:
        """
        Given a text, generates (start, end) spans of sentences
        in the text.
        """
        ...

    def sentences_from_text(self, text: str, realign_boundaries: bool = ...) -> List[str]:
        """
        Given a text, generates the sentences in that text by only
        testing candidate sentence breaks. If realign_boundaries is
        True, includes in the sentence closing punctuation that
        follows the period.
        """
        ...

    def text_contains_sentbreak(self, text: str) -> bool:
        """
        Returns True if the given text includes a sentence break.
        """
        ...

    def sentences_from_text_legacy(self, text: str) -> Iterator[str]:
        """
        Given a text, generates the sentences in that text. Annotates all
        tokens, rather than just those with possible sentence breaks. Should
        produce the same results as ``sentences_from_text``.
        """
        ...

    def sentences_from_tokens(self, tokens: Iterator[PunktToken]) -> Iterator[PunktToken]:
        """
        Given a sequence of tokens, generates lists of tokens, each list
        corresponding to a sentence.
        """
        ...

    def dump(self, tokens: Iterator[PunktToken]) -> None:
        ...

    PUNCTUATION = ...


class PunktTokenizer(PunktSentenceTokenizer):
    """
    Punkt Sentence Tokenizer that loads/saves its parameters from/to data files
    """
    def __init__(self, lang=...) -> None:
        ...

    def load_lang(self, lang=...): # -> None:
        ...

    def save_params(self): # -> None:
        ...



def load_punkt_params(lang_dir): # -> PunktParameters:
    ...

def save_punkt_params(params, dir=...): # -> None:
    ...

DEBUG_DECISION_FMT = ...
def format_debug_decision(d): # -> LiteralString:
    ...

def demo(text, tok_cls=..., train_cls=...): # -> None:
    """Builds a punkt model and applies it to the same text"""
    ...
